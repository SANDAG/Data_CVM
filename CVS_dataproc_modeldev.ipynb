{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDAG CV and TNC Survey Analysis\n",
    "### CV: Commercial Vehicles\n",
    "### TNC: Transportation Network Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the cell below, specify which data set to process:\n",
    "* 'CV' for Establishment surveys\n",
    "* 'TNC' for TNC surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate what dataset is to be processed. It should be either 'CV' or 'TNC'.\n",
    "dataset = 'CV'\n",
    "# dataset = 'TNC'\n",
    "\n",
    "# Specify value of time for different vehicle sizes, [Light, Medium, Heavy]. Unit is $ per hour.\n",
    "vot = [67, 68, 89]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import datetime as dt\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import openmatrix as omx\n",
    "import gc\n",
    "\n",
    "# from matplotlib.ticker import PercentFormatter\n",
    "# from scipy import stats  # to get inverse of '.quantile()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_date = datetime.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Trip Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trip:\n",
    "    registry = []\n",
    "\n",
    "    def __init__(self):\n",
    "        Trip.registry.append(self)\n",
    "        self.index = -1\n",
    "        self.o_act = 0\n",
    "        self.o_place_type = 0\n",
    "        self.o_place_name = ''\n",
    "        self.o_address = ''\n",
    "        self.o_lon = 0\n",
    "        self.o_lat = 0\n",
    "        self.o_taz = -1\n",
    "        self.d_act = 0\n",
    "        self.d_place_type = 0\n",
    "        self.d_place_name = ''\n",
    "        self.d_address = ''\n",
    "        self.d_lon = 0\n",
    "        self.d_lat = 0\n",
    "        self.d_taz = -1\n",
    "        self.trip_dist = 0\n",
    "        self.o_dt = dt.datetime(1982, 1, 1, 0, 0, 0)\n",
    "        self.d_at = dt.datetime(1982, 1, 1, 0, 0, 0)\n",
    "        self.nt_dt = dt.datetime(1982, 1, 1, 0, 0, 0)\n",
    "        self.travel_time = 0    # in seconds\n",
    "        self.stop_duration = 0  # in seconds\n",
    "        self.cargo_pickup = -1\n",
    "        self.cargo_delivery = -1\n",
    "        self.travel_date = dt.date(1982, 1, 1)\n",
    "        self.last_trip = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a021468",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.getcwd().replace(\"\\\\02 Scripts\", \"\")\n",
    "\n",
    "# To run the script on your computer, update 'project path' above with script folder address on your computer and\n",
    "# then uncomment it.\n",
    "in_data_dir  = '01 Inputs'\n",
    "out_data_dir = '03 Outputs'\n",
    "\n",
    "lu_file = 'Lookups_v8.xlsx'\n",
    "lu_path = os.path.join(project_path, in_data_dir, lu_file)\n",
    "\n",
    "skims_dir = r'C:\\Users\\jgliebe\\OneDrive - Cambridge Systematics\\Documents - PROJ SANDAG Commercial Vehicle & Heavy Truck Model Update\\_Shared_CSTeam\\Task03_DataID_Review\\ABM3\\Skims'\n",
    "\n",
    "if dataset == 'TNC':\n",
    "    in_data_file = 'TNC Travel Survey_Data Submittal_1-19-23.xlsx'\n",
    "else:\n",
    "#     in_data_file = 'SANDAG 2022 CV DataBase & Dictionaires_03_03_2023.xlsx'\n",
    "    in_data_file = 'SANDAG 2022 CV DataBase, Revised.xlsx'  # Cleaned up some data formatting\n",
    "in_data_path = os.path.join(project_path, in_data_dir, in_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ac766",
   "metadata": {},
   "source": [
    "#### Read the Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = pd.read_excel(lu_path, header=0, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7483b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['Segment Codes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_at = lookups['Activity Type']  # .copy()\n",
    "# lookup_at.drop(columns='count', inplace=True)\n",
    "\n",
    "asl = {1: 'S', 2: 'P', 3: 'D', 4: 'M', 5: 'B', 6: 'H'} # asl = Activity Segment Letter\n",
    "lookup_at['act_seg_let'] = lookup_at['act_seg_code'].map(asl)\n",
    "\n",
    "dic_at = {}\n",
    "for i, row in lookup_at.iterrows():\n",
    "    dic_at[row['activity_type_code']] = row['act_seg_let'], row['act_seg_name']\n",
    "\n",
    "lookup_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f13774",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_pt = lookups['Place Type']\n",
    "# lookup_pt.drop(columns='count', inplace=True)\n",
    "\n",
    "dic_pt = {}\n",
    "dic_pt2 = {}\n",
    "for i, row in lookup_pt.iterrows():\n",
    "    dic_pt[row['place_type_code']] = row['plc_seg_code'], row['plc_seg_name']\n",
    "    dic_pt2[row['plc_seg_code']] = row['plc_seg_name']\n",
    "\n",
    "lookup_pt # .head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b47d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_ind = lookups['Industries']\n",
    "lookup_ind #.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_tnc = lookups['TNC Categories']  # .copy()\n",
    "\n",
    "dic_tnc_names = {}\n",
    "dic_tnc_categories = {}\n",
    "for i, row in lookup_tnc.iterrows():\n",
    "    dic_tnc_names[row['company_name']] = row['company_name_alt']\n",
    "    dic_tnc_categories[row['company_name_alt']] = row['TNC_IndCat3']\n",
    "\n",
    "lookup_tnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_cv_estab = lookups['CV Estab TAZ']\n",
    "lookup_cv_estab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_tnc_estab = lookups['TNC Estab TAZ']  # .copy()\n",
    "lookup_tnc_estab.loc[lookup_tnc_estab['estab_taz'].isnull(), 'estab_taz'] = -1 # Could've been done with fillna too.\n",
    "lookup_tnc_estab['estab_taz'] = lookup_tnc_estab['estab_taz'].astype(int)\n",
    "lookup_tnc_estab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_cv_estab_replace = lookups['CV Rt Rplcmnt Estab TAZ'] #.copy()\n",
    "lookup_cv_estab_replace.insert(1, 'Date', 0)\n",
    "lookup_cv_estab_replace.insert(2, 'Veh', 0)\n",
    "lookup_cv_estab_replace['Date'] = pd.to_datetime(lookup_cv_estab_replace['route_id'].astype(str).str[:8]) #.astype(int)\n",
    "lookup_cv_estab_replace['Veh'] = lookup_cv_estab_replace['route_id'].astype(str).str[-4:].astype(int)\n",
    "\n",
    "lookup_cv_estab_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LogisticsNodes\n",
    "lookup_logistics = lookups['LogisticsNodes']\n",
    "lookup_logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb7b1e",
   "metadata": {},
   "source": [
    "### Read Skim Matrices -- two methods, choose one\n",
    "1. Read OMX files, process, and save as pickle files -- do this only if the pickle files do not already exist or new raw data skims are wanted.\n",
    "2. Read the pickle files directly, assuming they exist. This is 100 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9985eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There are 4,947 TAZs and 24,321 MGRAs in the shapefiles.\n",
    "# The OMX files contain info about TAZs, not MGRAs.\n",
    "dic_veh_size = {1: 'L', 2: 'M', 3: 'H'}\n",
    "dic_tod = {1: 'EA', 2: 'AM', 3: 'MD', 4: 'PM', 5: 'EV'}\n",
    "fns = [f'traffic_skims_{v}.omx' for v in dic_tod.values()] # fns = file names\n",
    "# print(fns)\n",
    "\n",
    "# Periods:\n",
    "# Early:   3AM     6AM\n",
    "# AM Peak: 6AM     9AM\n",
    "# Midday:  9AM     3:30PM\n",
    "# PM Peak: 3:30PM  7PM\n",
    "# Late:    7PM     3AM\n",
    "\n",
    "dist_dfs = [[None for j in range(len(dic_veh_size))] for i in range(len(dic_tod))]\n",
    "# 'dist_dfs' is a list of lists that stores distance dataframes.\n",
    "# Each row is for a time of day, and each column is a vehicle size.\n",
    "# Note: Indices start from zero.\n",
    "\n",
    "time_dfs = [[None for j in range(len(dic_veh_size))] for i in range(len(dic_tod))]\n",
    "toll_dfs = [[None for j in range(len(dic_veh_size))] for i in range(len(dic_tod))]\n",
    "g_tt_dfs = [[None for j in range(len(dic_veh_size))] for i in range(len(dic_tod))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e35f04",
   "metadata": {},
   "source": [
    "#### Method 1: Read skims for OMX files, process, and save as pickle files. (Slow)\n",
    "Change below cell to \"code\" before running"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3e36430",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Read in from OMX files if files not already in skim folder (slow)\n",
    "for tod_code in dic_tod.keys():\n",
    "    fn = fns[tod_code-1]\n",
    "    path = os.path.join(skims_dir, fn)\n",
    "    print(fn)\n",
    "    with omx.open_file(path) as f:\n",
    "#         for matrix in list(f.list_matrices()): print(matrix)\n",
    "        for size_code in dic_veh_size.keys():\n",
    "            dist_dfs[tod_code-1][size_code-1] = \\\n",
    "            pd.DataFrame(np.array(f[f'TRK_{dic_veh_size[size_code]}_DIST__{dic_tod[tod_code]}']))\n",
    "            dist_dfs[tod_code-1][size_code-1] = (dist_dfs[tod_code-1][size_code-1] * 10).astype('int32')\n",
    "\n",
    "            time_dfs[tod_code-1][size_code-1] = \\\n",
    "            pd.DataFrame(np.array(f[f'TRK_{dic_veh_size[size_code]}_TIME__{dic_tod[tod_code]}']))\n",
    "            time_dfs[tod_code-1][size_code-1] = (time_dfs[tod_code-1][size_code-1] * 100).astype('int32') / 100\n",
    "\n",
    "            toll_dfs[tod_code-1][size_code-1] = \\\n",
    "            pd.DataFrame(np.array(f[f'TRK_{dic_veh_size[size_code]}_TOLLCOST__{dic_tod[tod_code]}']))\n",
    "            toll_dfs[tod_code-1][size_code-1] = toll_dfs[tod_code-1][size_code-1].astype('int32')\n",
    "\n",
    "            fac = 100 * vot[size_code-1] / 60   # cents/min\n",
    "            g_tt_dfs[tod_code-1][size_code-1] = time_dfs[tod_code-1][size_code-1].\\\n",
    "            add(toll_dfs[tod_code-1][size_code-1] / fac)\n",
    "            g_tt_dfs[tod_code-1][size_code-1] = (g_tt_dfs[tod_code-1][size_code-1] * 100).astype('int32') / 100\n",
    "\n",
    "\n",
    "# Write Out the Read TAZ Distance Skim Matrices in 15 Pickle Files - Distances Are in Tenth of Miles.\n",
    "for i in range(len(dic_tod)):\n",
    "    for j in range(len(dic_veh_size)):\n",
    "        out_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_10th_of_Miles.pkl'\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Distance Skims')\n",
    "        if not os.path.exists(out_path):\n",
    "           os.makedirs(out_path)\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Distance Skims', out_file)\n",
    "        dist_dfs[i][j].to_pickle(out_path)\n",
    "\n",
    "# Write out the read TAZ travel time, toll, and generalized travel time skim matrices.\n",
    "for i in range(len(dic_tod)):\n",
    "    for j in range(len(dic_veh_size)):\n",
    "        out_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_TT_Minutes.pkl'\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Travel Time Skims')\n",
    "        if not os.path.exists(out_path):\n",
    "           os.makedirs(out_path)\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Travel Time Skims', out_file)\n",
    "        time_dfs[i][j].to_pickle(out_path)\n",
    "\n",
    "        out_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_Toll_Cents.pkl'\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Toll Skims')\n",
    "        if not os.path.exists(out_path):\n",
    "           os.makedirs(out_path)\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Toll Skims', out_file)\n",
    "        toll_dfs[i][j].to_pickle(out_path)\n",
    "\n",
    "        out_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_GenTT_Minutes.pkl'\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Generalized TT Skims')\n",
    "        if not os.path.exists(out_path):\n",
    "           os.makedirs(out_path)\n",
    "        out_path = os.path.join(project_path, out_data_dir, 'Generalized TT Skims', out_file)\n",
    "        g_tt_dfs[i][j].to_pickle(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017bf6f",
   "metadata": {},
   "source": [
    "#### Method 2: Read already processed skims from pickle files. (Fast)\n",
    "Change below cell to \"code\" before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read skim matrices from pickle files (fast load)\n",
    "for i in range(len(dic_tod)):\n",
    "    for j in range(len(dic_veh_size)):\n",
    "        in_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_10th_of_miles.pkl'\n",
    "        in_path = os.path.join(project_path, out_data_dir, 'Distance Skims')\n",
    "        if not os.path.exists(in_path):\n",
    "           print(f\"Input skims file path not found: \\n{in_path}\")\n",
    "        in_path = os.path.join(project_path, out_data_dir, 'Distance Skims', in_file)\n",
    "        dist_dfs[i][j] = pd.read_pickle(in_path)\n",
    "\n",
    "        if i == 2:\n",
    "            in_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_TT_Minutes.pkl'\n",
    "            in_path = os.path.join(project_path, out_data_dir, 'Travel Time Skims')\n",
    "            if not os.path.exists(in_path):\n",
    "               print(f\"Input skims file path not found: \\n{in_path}\")\n",
    "            in_path = os.path.join(project_path, out_data_dir, 'Travel Time Skims', in_file)\n",
    "            time_dfs[i][j] = pd.read_pickle(in_path)\n",
    "\n",
    "            in_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_Toll_Cents.pkl'\n",
    "            in_path = os.path.join(project_path, out_data_dir, 'Toll Skims')\n",
    "            if not os.path.exists(in_path):\n",
    "               print(f\"Input skims file path not found: \\n{in_path}\")\n",
    "            in_path = os.path.join(project_path, out_data_dir, 'Toll Skims', in_file)\n",
    "            toll_dfs[i][j] = pd.read_pickle(in_path)\n",
    "\n",
    "            in_file = f'{3*i+j+1:02}_{dic_tod[i+1]}_{dic_veh_size[j+1]}_GenTT_Minutes.pkl'\n",
    "            in_path = os.path.join(project_path, out_data_dir, 'Generalized TT Skims')\n",
    "            if not os.path.exists(in_path):\n",
    "               print(f\"Input skims file path not found: \\n{in_path}\")\n",
    "            in_path = os.path.join(project_path, out_data_dir, 'Generalized TT Skims', in_file)\n",
    "            g_tt_dfs[i][j] = pd.read_pickle(in_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224129ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Example 1: Distance from TAZ 5 to TAZ 19 in AM peak for medium trucks is: {dist_dfs[1][1].at[320, 344]/10:} miles\\n')\n",
    "print(f'Example 2: Distance from TAZ 321 to TAZ 345 in mid-day for light trucks is: {dist_dfs[2][0].iloc[320, 344]/10:} miles,',\n",
    "      '\\n', f'          Travel time is: {time_dfs[2][0].iloc[320, 344]} minutes, and', '\\n',\n",
    "      f'          toll is: {toll_dfs[2][0].iloc[320, 344]} cents.\\n')\n",
    "print(f'Example 3: Fastest route between TAZ 3901 and TAZ 4233 uses expressway 125, which is a FasTrak toll road.',\n",
    "      f'\\n           Distance from TAZ 3901 to TAZ 4233 in mid-day for heavy trucks is: {dist_dfs[2][2].iloc[3900, 4232]/10:} miles,',\n",
    "      '\\n', f'          Travel time is: {time_dfs[2][2].iloc[3900, 4232]} minutes,\\n',\n",
    "      f'          Toll is: {toll_dfs[2][2].iloc[3900, 4232]} cents, and\\n'\n",
    "      f'           Generalized travel time is: {g_tt_dfs[2][2].iloc[3900, 4232]} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a90f35",
   "metadata": {},
   "source": [
    "#### Read Stop, Establishment, and Vehicle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa553fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the survey data.\n",
    "df_original = pd.read_excel(in_data_path, header=0, sheet_name=None)\n",
    "# df_original is a dictionary of Dfs. Keys are sheetnames, and values are the dataframes in those worksheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If all we were to do was to read the stop data:\n",
    "# df_stops = pd.read_excel(in_data_path, header=0, sheet_name='Trip Data')\n",
    "# df_stops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops = df_original['Trip Data'].copy()\n",
    "df_estab = df_original['Establishment Data'].copy()\n",
    "df_veh = df_original['Vehicle Data'].copy()\n",
    "df_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635302c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9ccdf",
   "metadata": {},
   "source": [
    "#### Explore the Vehicle Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504599ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'vehicle_classification' codes according to data dictionary:\n",
    "# 1: Passenger Car or Motorcycle\n",
    "# 2: Pick-up Truck (4 wheels)\n",
    "# 3: Van (Cargo/Minivan) (4 wheels)\n",
    "# 4: Buses\n",
    "# 5: Single Unit 2-axle\n",
    "# 6: Single Unit 3-axle\n",
    "# 7: Single Unit 4-axle\n",
    "# 8: Semi (all Tractor-Trailer combinations)\n",
    "# 96:Other (please specify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73826888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veh['vehicle_classification'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36877f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'CV':\n",
    "    print(df_veh['Vehicle Type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'CV':\n",
    "    pvt_axle_vtype = pd.pivot_table(df_veh, values='id', index='vehicle_classification', columns='Vehicle Type',\n",
    "                             aggfunc='count') #, sort=True) For sort, default is True.\n",
    "    print(pvt_axle_vtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversion dictionary from veh_classification to veh_size.\n",
    "# This is necessary because the TNC dataset doesn't have the 'Vehicle Type' column - this script\n",
    "# needs to work for both CV and the TNC datasets.\n",
    "d1 = {i: 'LCV' for i in range(1, 4)}\n",
    "d2 = {i: 'SUT' for i in range(4, 8)}\n",
    "d3 = {8: 'MUT'}\n",
    "dic_veh_size = {**d1, **d2, **d3}\n",
    "dic_veh_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veh['veh_size'] = df_veh['vehicle_classification'].map(dic_veh_size)\n",
    "df_veh[['veh_size', 'vehicle_classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c2ac8",
   "metadata": {},
   "source": [
    "#### Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which columns to keep and rearrange.\n",
    "cols = ['company_id', 'vehicle_id', 'driver_id', 'trip_number', 'activity_type', 'placetype',\n",
    "        'location_placename', 'location_address', 'location_longitude', 'location_latitude', 'taz',\n",
    "        'arrival_time', 'departure_time', 'cargo_pickup', 'cargo_delivery', 'travel_date',\n",
    "        'participation_type', 'Most Likely Estimate Weight Factor']\n",
    "        # 'Lower Estimate Weight Factor',\n",
    "        # 'Upper Estimate Weight Factor'\n",
    "\n",
    "df_stops = df_stops[cols]\n",
    "\n",
    "# Sort the dataframe\n",
    "df_stops.sort_values(by=['company_id', 'vehicle_id', 'travel_date', 'trip_number'],\n",
    "                     inplace=True, ascending=True)\n",
    "df_stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56294a7",
   "metadata": {},
   "source": [
    "#### Attach Vehicle Type Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08002f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops = df_stops.merge(df_veh[['id', 'veh_size']], how='left', left_on='vehicle_id', right_on='id')\n",
    "df_stops.rename(columns={'veh_size': 'veh_type'}, inplace=True)\n",
    "df_stops.drop(columns='id', inplace=True)\n",
    "df_stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c35881",
   "metadata": {},
   "source": [
    "#### Attach Establishment TAZ Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_estab = lookup_cv_estab if dataset == 'CV' else lookup_tnc_estab\n",
    "df_stops = df_stops.merge(lookup_estab[['company_id', 'estab_taz']], how='left', on='company_id')\n",
    "df_stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164c7ff",
   "metadata": {},
   "source": [
    "#### Update establishment TAZ info for stops that have a replacement establishment TAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76738d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'CV':\n",
    "    for i, rowStop in df_stops.iterrows():\n",
    "        for j, rowRep in lookup_cv_estab_replace.iterrows():\n",
    "            if rowStop['travel_date']==rowRep['Date'] and rowStop['vehicle_id']==rowRep['Veh']:\n",
    "#                 print(i, j,\n",
    "#                       f\"{df_stops.at[i, 'estab_taz']} was replaced by {lookup_cv_estab_replace.at[j, 'NewEstab_TAZ']}\")\n",
    "                df_stops.at[i, 'estab_taz'] = lookup_cv_estab_replace.at[j, 'NewEstab_TAZ']\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd7c11",
   "metadata": {},
   "source": [
    "#### Create the Trip Dataframe Using the Stop Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4789621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_stops.copy()\n",
    "rename_dic = {\n",
    "    'activity_type':      'd_act',\n",
    "    'placetype':          'd_place_type',\n",
    "    'location_placename': 'd_place_name',\n",
    "    'location_address':   'd_address',\n",
    "    'location_longitude': 'd_lon',\n",
    "    'location_latitude':  'd_lat',\n",
    "    'taz':                'd_taz',\n",
    "    'arrival_time':       'd_arr_time',\n",
    "    'departure_time':     'next_trip_dep_time',\n",
    "    'Most Likely Estimate Weight Factor': 'expnsn_factor'\n",
    "}\n",
    "df_temp.rename(columns=rename_dic, inplace=True)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc97b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TNCs, set missing expansion factors to 1\n",
    "df_temp['expnsn_factor'] = df_temp['expnsn_factor'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of new columns that should be added.\n",
    "cols_to_add_1 = ['o_act', 'o_place_type', 'o_lon', 'o_lat', 'o_taz', 'o_dep_time',\n",
    "                 'travel_time', 'stop_duration',\n",
    "                 'o_act_seg', 'o_plc_seg',\n",
    "                 'd_act_seg', 'd_plc_seg',\n",
    "                 'trip_dist', 'orgn_to_hq_dist',\n",
    "                 'hq_taz'\n",
    "                ]\n",
    "d1 = dict.fromkeys(cols_to_add_1, 0)\n",
    "\n",
    "cols_to_add_2 = ['o_place_name', 'o_address',\n",
    "                 'o_act_seg_name', 'o_plc_seg_name',\n",
    "                 'd_act_seg_name', 'd_plc_seg_name',\n",
    "                 'headquarters'\n",
    "                ]\n",
    "d2 = dict.fromkeys(cols_to_add_2, \"\")\n",
    "\n",
    "d = {**d1, **d2}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new columns with them being initialized by the values of the above dictionary.\n",
    "df_temp = df_temp.assign(**d)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5908e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange.\n",
    "cols = ['company_id', 'vehicle_id', 'driver_id', 'trip_number',\n",
    "        'o_act_seg', 'o_act_seg_name', 'o_plc_seg', 'o_plc_seg_name',\n",
    "        'd_act_seg', 'd_act_seg_name', 'd_plc_seg', 'd_plc_seg_name',\n",
    "        'o_act', 'o_place_type', 'o_place_name', 'o_address', 'o_lon', 'o_lat', 'o_taz',\n",
    "        'd_act', 'd_place_type', 'd_place_name', 'd_address', 'd_lon', 'd_lat', 'd_taz',\n",
    "        'trip_dist', 'headquarters', 'hq_taz', 'orgn_to_hq_dist',\n",
    "        'o_dep_time', 'd_arr_time', 'next_trip_dep_time', 'travel_time', 'stop_duration',\n",
    "        'veh_type', 'cargo_pickup', 'cargo_delivery', 'travel_date',\n",
    "        'participation_type', 'expnsn_factor', 'estab_taz'\n",
    "       ]\n",
    "df_temp = df_temp[cols]\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add origin information from the previous row.\n",
    "dic_read_last = {\n",
    "    'o_act':        'd_act',\n",
    "    'o_place_type': 'd_place_type',\n",
    "    'o_place_name': 'd_place_name',\n",
    "    'o_address':    'd_address',\n",
    "    'o_lon':        'd_lon',\n",
    "    'o_lat':        'd_lat',\n",
    "    'o_taz':        'd_taz',\n",
    "    'o_dep_time':   'next_trip_dep_time',\n",
    "    'travel_date':  'travel_date'\n",
    "}\n",
    "\n",
    "for k, v in dic_read_last.items():\n",
    "    df_temp.loc[df_temp['trip_number']!=0, k] = df_temp[v].shift(1)\n",
    "\n",
    "df_temp['o_taz'] = df_temp['o_taz'].astype('int64')\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first record of each vehicle as it doesn't represent a trip, rather the\n",
    "# initial origin of the vehicle, whose critical info has already been stored in the next record.\n",
    "df_temp = df_temp.loc[df_temp['trip_number']!=0]\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d46da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the activity and place segment fields.\n",
    "df_temp['o_act_seg']      = df_temp['o_act'].map(lambda x: dic_at[x][0])\n",
    "df_temp['o_act_seg_name'] = df_temp['o_act'].map(lambda x: dic_at[x][1])\n",
    "df_temp['d_act_seg']      = df_temp['d_act'].apply(lambda x: dic_at[x][0])\n",
    "df_temp['d_act_seg_name'] = df_temp['d_act'].apply(lambda x: dic_at[x][1])\n",
    "\n",
    "df_temp['o_plc_seg']      = df_temp['o_place_type'].map(lambda x: dic_pt[x][0])\n",
    "df_temp['o_plc_seg_name'] = df_temp['o_place_type'].map(lambda x: dic_pt[x][1])\n",
    "df_temp['d_plc_seg']      = df_temp['d_place_type'].apply(lambda x: dic_pt[x][0])\n",
    "df_temp['d_plc_seg_name'] = df_temp['d_place_type'].apply(lambda x: dic_pt[x][1])\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.loc[(df_temp['d_act_seg']=='B')&(df_temp['d_taz']!=df_temp['estab_taz'])].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.groupby(['d_act_seg', 'd_act_seg_name']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada351b",
   "metadata": {},
   "source": [
    "##### Fix Activity Type 'Base' If Stop's TAZ Is Not Equal to Establishment TAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc[(df_temp['o_act_seg']=='B')&\n",
    "            (df_temp['o_taz']!=df_temp['estab_taz'])&\n",
    "            (df_temp['o_act'].isin([12])), ['o_act_seg', 'o_act_seg_name']\n",
    "           ] = 'P', 'Goods_Pickup'\n",
    "\n",
    "df_temp.loc[(df_temp['o_act_seg']=='B')&\n",
    "            (df_temp['o_taz']!=df_temp['estab_taz'])&\n",
    "            (df_temp['o_act'].isin([13])), ['o_act_seg', 'o_act_seg_name']\n",
    "           ] = 'D', 'Goods_Delivery'\n",
    "\n",
    "df_temp.loc[(df_temp['o_act_seg']=='B')&\n",
    "            (df_temp['o_taz']!=df_temp['estab_taz'])&\n",
    "            (~df_temp['o_act'].isin([12, 13])), ['o_act_seg', 'o_act_seg_name']\n",
    "           ] = 'M', 'Maintenance/Other'\n",
    "\n",
    "df_temp.loc[(df_temp['d_act_seg']=='B')&\n",
    "            (df_temp['d_taz']!=df_temp['estab_taz'])&\n",
    "            (df_temp['d_act'].isin([12])), ['d_act_seg', 'd_act_seg_name']\n",
    "           ] = 'P', 'Goods_Pickup'\n",
    "\n",
    "df_temp.loc[(df_temp['d_act_seg']=='B')&\n",
    "            (df_temp['d_taz']!=df_temp['estab_taz'])&\n",
    "            (df_temp['d_act'].isin([13])), ['d_act_seg', 'd_act_seg_name']\n",
    "           ] = 'D', 'Goods_Delivery'\n",
    "\n",
    "df_temp.loc[(df_temp['d_act_seg']=='B')&\n",
    "            (df_temp['d_taz']!=df_temp['estab_taz'])&\n",
    "            (~df_temp['d_act'].isin([12, 13])), ['d_act_seg', 'd_act_seg_name']\n",
    "           ] = 'M', 'Maintenance/Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.loc[df_temp['d_taz']!=df_temp['estab_taz']].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f00153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd69d03",
   "metadata": {},
   "source": [
    "##### Include the industry of establishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c32e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'CV':\n",
    "    df_temp = df_temp.merge(df_estab[['company_id', 'base_location_Industry Group']], how='left', on='company_id')\n",
    "    df_temp.rename(columns={'base_location_Industry Group': 'industry_code'}, inplace=True)\n",
    "    df_temp = df_temp.merge(lookup_ind[['industry_code', 'industry_group']], how='left', on='industry_code')\n",
    "    temp = df_temp.pop('industry_group')\n",
    "    df_temp.insert(1, 'industry_group', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omit_spaces(s):\n",
    "    return s.replace(' ', '')\n",
    "\n",
    "if dataset == 'TNC':\n",
    "    df_temp = df_temp.merge(df_estab[['company_id', 'company_name']], how='left', on='company_id')\n",
    "\n",
    "    # Some of the company names have leading or lagging spaces in their names.\n",
    "    df_temp['company_name'] = df_temp['company_name'].apply(str.strip).apply(str.lower).apply(omit_spaces)\n",
    "    df_temp['company_name'] = df_temp['company_name'].map(dic_tnc_names)\n",
    "\n",
    "    # Figure out the industry groups.\n",
    "    df_temp.insert(1, 'industry_group', \"\")\n",
    "    df_temp['industry_group'] = df_temp['company_name'].map(dic_tnc_categories)\n",
    "\n",
    "    df_temp['industry_group'] = 'TNC_' + df_temp['industry_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eeb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = df_temp.copy()\n",
    "del df_temp\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b47b8e",
   "metadata": {},
   "source": [
    "##### Clean the time fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10b94e",
   "metadata": {},
   "source": [
    "Glossary:<br>\n",
    "|Field          | Meaning|\n",
    "|:---------------|:--------------------|\n",
    "|str_td | Travel Date as String | \n",
    "|str_o_dt|      Departure Time from the Origin as String|\n",
    "|str_d_at |     Arrival Time at the Destination as String|\n",
    "|str_nt_dt |    Next Trip Departure Time as String|\n",
    "|o_dt       |   Departure Time from the Origin as TimeStamp|\n",
    "|d_at        |  Arrival Time at the Destination as TimeStamp|\n",
    "|nt_dt        | Next Trip Departure Time as TimeStamp|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_trips['travel_date'].dtypes)\n",
    "print(df_trips['o_dep_time'].dtypes)\n",
    "print(df_trips['d_arr_time'].dtypes)\n",
    "print(df_trips['next_trip_dep_time'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['str_td'] = df_trips['travel_date'].dt.strftime('%Y-%m-%d')    # str_td = travel date as string\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['o_dep_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'CV':\n",
    "    df_trips['str_o_dt'] =  df_trips['o_dep_time'].astype(str)\n",
    "    print(f\"Max length of column str_o_dt: {max(df_trips['str_o_dt'].str.len())}\")\n",
    "    print(f\"Min length of column str_o_dt: {min(df_trips['str_o_dt'].str.len())}\\n\")\n",
    "\n",
    "    df_trips['str_d_at'] =  df_trips['d_arr_time'].astype(str)\n",
    "    print(f\"Max length of column str_d_at: {max(df_trips['str_d_at'].str.len())}\")\n",
    "    print(f\"Min length of column str_d_at: {min(df_trips['str_d_at'].str.len())}\")\n",
    "    df_trips.loc[df_trips['str_d_at'].str.len()>8]\n",
    "    df_trips['str_d_at'] =df_trips['str_d_at'].str[-8:]\n",
    "    print(f\"Max length of column str_d_at: {max(df_trips['str_d_at'].str.len())}, after cleaning\")\n",
    "    print(f\"Min length of column str_d_at: {min(df_trips['str_d_at'].str.len())}, after cleaning\\n\")\n",
    "\n",
    "    df_trips['str_nt_dt'] =  df_trips['next_trip_dep_time'].astype(str)\n",
    "    print(f\"Max length of column str_nt_dt: {max(df_trips['str_nt_dt'].str.len())}\")\n",
    "    print(f\"Min length of column str_nt_dt: {min(df_trips['str_nt_dt'].str.len())}\")\n",
    "    df_trips.loc[df_trips['str_nt_dt'].str.len()<8].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'TNC':\n",
    "    df_trips['str_o_dt'] =  df_trips['o_dep_time'].astype(str)\n",
    "#     print('Before:\\n', df_trips['str_o_dt'].loc[~df_trips['str_o_dt'].str.len().isin([8, 15])])\n",
    "    df_trips['str_o_dt'] = df_trips['str_o_dt'].\\\n",
    "    apply(lambda x: x[11:] if x[:10]=='1900-01-01' else x)\n",
    "#     print('\\nAfter:\\n', df_trips['str_o_dt'].loc[~df_trips['str_o_dt'].str.len().isin([8, 15])])\n",
    "    df_trips['str_o_dt'].loc[df_trips['str_o_dt'].str.len()!=8]\n",
    "    df_trips['str_o_dt'].loc[df_trips['str_o_dt'].str.len()==8]\n",
    "    df_trips.loc[df_trips['str_o_dt'].str.len()!=8, 'str_o_dt'] = df_trips['str_o_dt'].str[:-7]\n",
    "    print(f\"\\n'o_dt' data are now clean: \" +\n",
    "          f\"{len(df_trips.loc[df_trips['str_o_dt'].str.len()==8]) == len(df_trips)}\\n\\n\")\n",
    "\n",
    "    df_trips['str_d_at'] =  df_trips['d_arr_time'].astype(str)\n",
    "#     print('Before:\\n', df_trips['str_d_at'].loc[~df_trips['str_d_at'].str.len().isin([8, 15])])\n",
    "    df_trips['str_d_at'] = df_trips['str_d_at'].\\\n",
    "    apply(lambda x: x[11:] if x[:10]=='1900-01-01' else x)\n",
    "#     print('\\nAfter:\\n', df_trips['str_d_at'].loc[~df_trips['str_d_at'].str.len().isin([8, 15])])\n",
    "    df_trips['str_d_at'].loc[df_trips['str_d_at'].str.len()!=8]\n",
    "    df_trips['str_d_at'].loc[df_trips['str_d_at'].str.len()==8]\n",
    "    df_trips.loc[df_trips['str_d_at'].str.len()!=8, 'str_d_at'] = df_trips['str_d_at'].str[:-7]\n",
    "    print(f\"\\n'd_at' data are now clean: \" +\n",
    "          f\"{len(df_trips.loc[df_trips['str_d_at'].str.len()==8]) == len(df_trips)}\")\n",
    "\n",
    "    df_trips['str_nt_dt'] =  df_trips['next_trip_dep_time'].astype(str)\n",
    "#     print('Before:\\n', df_trips['str_nt_dt'].loc[~df_trips['str_nt_dt'].str.len().isin([3, 8, 15])])\n",
    "    df_trips['str_nt_dt'] = df_trips['str_nt_dt'].\\\n",
    "    apply(lambda x: x[11:] if x[:10]=='1900-01-01' else x)\n",
    "#     print('\\nAfter:\\n', df_trips['str_nt_dt'].loc[~df_trips['str_nt_dt'].str.len().isin([3, 8, 15])])\n",
    "    df_trips['str_nt_dt'].loc[~df_trips['str_nt_dt'].str.len().isin([3, 8])]\n",
    "    df_trips['str_nt_dt'].loc[df_trips['str_nt_dt'].str.len()==8]\n",
    "    df_trips.loc[df_trips['str_nt_dt'].str.len()==15, 'str_nt_dt'] = df_trips['str_nt_dt'].str[:-7]\n",
    "    actual_times = len(df_trips.loc[df_trips['str_nt_dt'].str.len()==8])\n",
    "    nans = len(df_trips.loc[df_trips['str_nt_dt'].str.len()==3])\n",
    "    print(f\"\\n'nt_dt' data are now clean: \" +\n",
    "          f\"{actual_times+nans==len(df_trips)}\\n\")\n",
    "    print(f'Number of recorded times: {actual_times}')\n",
    "    print(f'Number of NANs: {nans}')\n",
    "    print(f'Total trips in the dataframe: {len(df_trips)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2059988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['o_dt'] = df_trips['str_td'] + ' ' + df_trips['str_o_dt']\n",
    "df_trips['o_dt'] = pd.to_datetime(df_trips['o_dt'])\n",
    "print(type(df_trips['o_dt']))\n",
    "print(type(df_trips['o_dt'][0]))\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d078110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['d_at'] = df_trips['str_td'] + ' ' + df_trips['str_d_at']\n",
    "df_trips['d_at'] = pd.to_datetime(df_trips['d_at'])\n",
    "print(type(df_trips['d_at']))\n",
    "print(type(df_trips['d_at'][0]))\n",
    "print(f\"Number of NAs in this column: {df_trips['d_at'].isna().sum()}\")\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['nt_dt'] = df_trips['str_td'] + ' ' + df_trips['str_nt_dt']\n",
    "df_trips['nt_dt'] = pd.to_datetime(df_trips['nt_dt'], errors='coerce')\n",
    "print(type(df_trips['nt_dt']))\n",
    "print(type(df_trips['nt_dt'][0]))\n",
    "print(f\"Number of NaTs in this column: {df_trips['nt_dt'].isna().sum()}\")\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deac74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests:\n",
    "print(df_trips['d_at'][0] - df_trips['o_dt'][0])\n",
    "print(df_trips['d_at'][0] > df_trips['o_dt'][0])\n",
    "print(df_trips['nt_dt'][3] - df_trips['d_at'][3])\n",
    "print(df_trips['nt_dt'][0] + pd.to_timedelta(1, unit='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c11337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure events are in chronological order.\n",
    "print(len(df_trips.loc[df_trips['d_at']<df_trips['o_dt']]))\n",
    "df_trips.loc[df_trips['d_at']<df_trips['o_dt'], 'd_at'] = df_trips['d_at'] + pd.to_timedelta(1, unit='days')\n",
    "print(len(df_trips.loc[df_trips['d_at']<df_trips['o_dt']]), '\\n')\n",
    "\n",
    "print(len(df_trips.loc[df_trips['nt_dt']<df_trips['d_at']]))\n",
    "df_trips.loc[df_trips['nt_dt']<df_trips['d_at'], 'nt_dt'] = df_trips['nt_dt'] + pd.to_timedelta(1, unit='days')\n",
    "print(len(df_trips.loc[df_trips['nt_dt']<df_trips['d_at']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find travel time and stop duration in minutes.\n",
    "# df_trips['travel_time'] = ((df_trips['d_at'] - df_trips['o_dt']).dt.seconds).astype(int)\n",
    "# df_trips['stop_duration'] = ((df_trips['nt_dt'] - df_trips['d_at']).dt.seconds).round().astype('Int32') #, errors='ignore'\n",
    "df_trips['travel_time'] = (((df_trips['d_at'] - df_trips['o_dt']).dt.seconds) / 60).astype(int)\n",
    "df_trips['stop_duration'] = (((df_trips['nt_dt'] - df_trips['d_at']).dt.seconds) / 60).round().astype('Int32') #, errors='ignore'\n",
    "# Note the capitalized 'Int32' in the line above - as opposed to 'int32'.\n",
    "# print(f\"Now, the datatype for the 'stop_duration' column is kind of weird: {df_trips['stop_duration'].dtypes}, rather than simply int32.\")\n",
    "# print('We need to live with this because we want to allow <NA> values in this column.')\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw away unnecessary time fields.\n",
    "rmv_fields = ['o_dep_time', 'd_arr_time', 'next_trip_dep_time',\n",
    "              'str_td', 'str_o_dt', 'str_d_at', 'str_nt_dt']\n",
    "all_fields = df_trips.columns.to_list()\n",
    "keep_cols = [f for f in all_fields if not f in rmv_fields]\n",
    "\n",
    "df_trips = df_trips[keep_cols]\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e7654",
   "metadata": {},
   "source": [
    "##### Mark the last trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab435fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['last_trip'] = False\n",
    "df_trips.loc[df_trips['trip_number'].shift(-1)==1, 'last_trip'] = True\n",
    "df_trips.at[df_trips.index[-1], 'last_trip'] = True\n",
    "df_trips.tail()\n",
    "df_trips.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b79226",
   "metadata": {},
   "source": [
    "##### Find the trip distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['tod'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3561e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate at what time of day the trip has started.\n",
    "def find_tod(timestamp):\n",
    "    if timestamp.time() < dt.time(3):\n",
    "        return 'EV' # Late: 7PM to 3AM\n",
    "    elif timestamp.time() < dt.time(6):\n",
    "        return 'EA' # Early: 3AM to 6AM\n",
    "    elif timestamp.time() < dt.time(9):\n",
    "        return 'AM' # AM Peak: 6AM to 9AM\n",
    "    elif timestamp.time() < dt.time(15, 30):\n",
    "        return 'MD' # Midday: 9AM to 3:30PM\n",
    "    elif timestamp.time() < dt.time(19):\n",
    "        return 'PM' # PM Peak: 3:30PM to 7pm\n",
    "    elif timestamp.time() >= dt.time(19):\n",
    "        return 'EV' # Late: 7PM to 3AM\n",
    "\n",
    "df_trips['tod'] = df_trips['o_dt'].apply(find_tod)\n",
    "# df_trips[['o_dt', 'tod']].loc[(df_trips['o_dt'].dt.time>=dt.time(0)) & (df_trips['o_dt'].dt.time<dt.time(3))].head(10)\n",
    "# df_trips[['o_dt', 'tod']].loc[(df_trips['o_dt'].dt.time>=dt.time(3)) & (df_trips['o_dt'].dt.time<dt.time(6))].head(10)\n",
    "# df_trips[['o_dt', 'tod']].loc[(df_trips['o_dt'].dt.time>=dt.time(6)) & (df_trips['o_dt'].dt.time<dt.time(9))].head(10)\n",
    "# df_trips[['o_dt', 'tod']].loc[(df_trips['o_dt'].dt.time>=dt.time(9)) & (df_trips['o_dt'].dt.time<dt.time(15, 30))].head(10)\n",
    "# df_trips[['o_dt', 'tod']].loc[(df_trips['o_dt'].dt.time>=dt.time(15,30)) & (df_trips['o_dt'].dt.time<dt.time(19))].head(10)\n",
    "df_trips[['o_dt', 'tod']].loc[(df_trips['o_dt'].dt.time>=dt.time(19)) & (df_trips['o_dt'].dt.time<dt.time(23, 59, 59, 999999))].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Example: Distance from TAZ 5 to TAZ 19 in AM peak for medium trucks is: {dist_dfs[1][1].at[4,18]/10:} miles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48455b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_v = {'LCV': 0, 'SUT': 1, 'MUT': 2}\n",
    "dic_tod = {'EA': 0, 'AM': 1, 'MD':2, 'PM':3, 'EV':4}\n",
    "\n",
    "def find_dist(o_taz, d_taz, vs, tod): # vs is vehicle size and tod is time of day.\n",
    "    if o_taz==-1 or d_taz==-1: return None\n",
    "    return dist_dfs[tod][vs].at[o_taz-1, d_taz-1] / 10\n",
    "\n",
    "df_trips['trip_dist'] = df_trips.apply(lambda x: find_dist(x['o_taz'], x['d_taz'],\n",
    "                                                           dic_v[x['veh_type']],\n",
    "                                                           dic_tod[x['tod']]), axis=1)\n",
    "\n",
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd59b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips[['o_taz','d_taz','veh_type','tod']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106d674",
   "metadata": {},
   "source": [
    "##### Find the trip generalized travel time from the establishment base to the trip destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51553ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Example: Generalized travel time from TAZ 3901 to TAZ 4233 in mid-day for heavy trucks is: {g_tt_dfs[2][2].iloc[3900, 4232]} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_v = {'LCV': 0, 'SUT': 1, 'MUT': 2}\n",
    "for i, row in df_trips.iterrows():\n",
    "    if row['estab_taz'] == -1 or row['d_taz'] == -1: continue\n",
    "    df_trips.at[i, 'toll_in_cents_from_base'] = toll_dfs[2][dic_v[row['veh_type']]].at[row['estab_taz']-1, row['d_taz']-1]\n",
    "    df_trips.at[i, 'gen_tt_from_base'] = g_tt_dfs[2][dic_v[row['veh_type']]].at[row['estab_taz']-1, row['d_taz']-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157edf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'Generalized TT for {dataset}.xlsx'\n",
    "out_path = os.path.join(project_path, out_data_dir, out_file)\n",
    "df_trips.to_excel(out_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5324817",
   "metadata": {},
   "source": [
    "##### Calculate average generalized travel time by industry by destination activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_trips.loc[df_trips['gen_tt_from_base'].isnull()]))\n",
    "df_trips.loc[df_trips['gen_tt_from_base'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['industry_group', 'd_act_seg', 'd_plc_seg_name', 'veh_type', 'expnsn_factor', 'gen_tt_from_base']\n",
    "df_gtt = df_trips[cols].copy()\n",
    "df_gtt = df_gtt.loc[~df_gtt['gen_tt_from_base'].isnull()]\n",
    "df_gtt = df_gtt.loc[df_gtt['d_act_seg'].isin(['P', 'D', 'S'])]\n",
    "df_gtt.rename(columns={'d_act_seg': 'purpose'}, inplace=True)\n",
    "df_gtt['purpose'] = df_gtt['purpose'].apply(lambda x: 'Service' if x=='S' else 'Goods')\n",
    "# df_gtt.insert(4, 'count', 1)\n",
    "df_gtt.insert(3, 'customer', 'Business')\n",
    "df_gtt.loc[df_gtt['d_plc_seg_name']=='Residential', 'customer'] = 'Resident'\n",
    "df_gtt.drop(columns='d_plc_seg_name', inplace=True)\n",
    "df_gtt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee36df",
   "metadata": {},
   "source": [
    "###### Unweighted generalized travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb76277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt_unw1 = df_gtt.groupby(['industry_group', 'purpose', 'customer', 'veh_type']).size().rename('sample_size').reset_index()\n",
    "df_gtt_unw1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt_unw2 = df_gtt.groupby(['industry_group', 'purpose', 'customer', 'veh_type'])['gen_tt_from_base']\\\n",
    ".mean().reset_index()\n",
    "df_gtt_unw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt_unw = pd.merge(df_gtt_unw1, df_gtt_unw2, on=['industry_group', 'purpose', 'customer', 'veh_type']).dropna()\n",
    "df_gtt_unw['product'] = df_gtt_unw['sample_size'] * df_gtt_unw['gen_tt_from_base']\n",
    "df_gtt_unw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc230a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_gtt_unw.groupby(['industry_group', 'purpose', 'customer'])['sample_size'].sum()\n",
    "df_avg_gtt_unw = ((df_gtt_unw.groupby(['industry_group', 'purpose', 'customer'])['product'].sum()/s)\\\n",
    "                  .rename('avg_gen_tt_unweighted')).reset_index()\n",
    "df_avg_gtt_unw = s.reset_index().merge(df_avg_gtt_unw, on=['industry_group', 'purpose', 'customer'] )\n",
    "df_avg_gtt_unw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3907c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'Generalized TT for {dataset} Segments, Unweighted.xlsx'\n",
    "out_path = os.path.join(project_path, out_data_dir, out_file)\n",
    "df_avg_gtt_unw.to_excel(out_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002d2a3",
   "metadata": {},
   "source": [
    "###### Weighted generalized travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt2 = df_gtt.copy()\n",
    "df_gtt2['expanded_tt'] = df_gtt2.expnsn_factor * df_gtt2.gen_tt_from_base\n",
    "df_gtt2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt2_wei0 = df_gtt2.groupby(['industry_group', 'purpose', 'customer', 'veh_type']).size().rename('sample_size').reset_index()\n",
    "df_gtt2_wei0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdae711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt2_wei1 = df_gtt2.groupby(['industry_group', 'purpose', 'customer', 'veh_type'])['expnsn_factor']\\\n",
    ".sum().rename('pop_size').reset_index()\n",
    "df_gtt2_wei1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt2_wei2 = df_gtt2.groupby(['industry_group', 'purpose', 'customer', 'veh_type'])['expanded_tt']\\\n",
    ".sum().reset_index()\n",
    "df_gtt2_wei2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa344136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtt2_wei = pd.merge(df_gtt2_wei0, df_gtt2_wei1, on=['industry_group', 'purpose', 'customer', 'veh_type'])\n",
    "df_gtt2_wei = pd.merge(df_gtt2_wei, df_gtt2_wei2, on=['industry_group', 'purpose', 'customer', 'veh_type']).dropna()\n",
    "# df_gtt2_wei['product'] = df_gtt2_wei['pop_size'] * df_gtt2_wei['expanded_tt']\n",
    "df_gtt2_wei.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz= df_gtt2_wei.groupby(['industry_group', 'purpose', 'customer'])['sample_size'].sum()\n",
    "s = df_gtt2_wei.groupby(['industry_group', 'purpose', 'customer'])['pop_size'].sum()\n",
    "df_avg_gtt2_wei = ((df_gtt2_wei.groupby(['industry_group', 'purpose', 'customer'])['expanded_tt'].sum()/s)\\\n",
    "                  .rename('avg_gen_tt_weighted')).reset_index()\n",
    "s = sz.reset_index().merge(s, on=['industry_group', 'purpose', 'customer'] )\n",
    "df_avg_gtt2_wei = s.reset_index().merge(df_avg_gtt2_wei, on=['industry_group', 'purpose', 'customer'] )\n",
    "df_avg_gtt2_wei.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'Generalized TT for {dataset} Segments, Weighted.xlsx'\n",
    "out_path = os.path.join(project_path, out_data_dir, out_file)\n",
    "df_avg_gtt2_wei.to_excel(out_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d122cd7",
   "metadata": {},
   "source": [
    "#### Create Summaries from the Trip Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac53880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted:\n",
    "AvgStop1 = df_trips[df_trips['stop_duration']>0].groupby(['d_act_seg_name', 'industry_group']).size().rename('count')\n",
    "AvgStop2 = df_trips[df_trips['stop_duration']>0].groupby(['d_act_seg_name', 'industry_group'])['stop_duration']\\\n",
    ".mean().rename('stop_duration_minutes').round()\n",
    "\n",
    "# Weighted:\n",
    "df_trips.loc[df_trips['stop_duration']>0, 'dur_by_weight'] = df_trips['stop_duration'] * df_trips['expnsn_factor']\n",
    "AvgStop1w = df_trips[df_trips['stop_duration']>0].groupby(['d_act_seg_name', 'industry_group'])['expnsn_factor'].sum().rename('weighted_count')\n",
    "AvgStop2w = (df_trips[df_trips['stop_duration']>0].groupby(['d_act_seg_name', 'industry_group'])['dur_by_weight'].sum()/\\\n",
    "            df_trips[df_trips['stop_duration']>0].groupby(['d_act_seg_name', 'industry_group'])['expnsn_factor'].sum()).\\\n",
    "            rename('stop_duration_minutes').round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trips[df_trips['stop_duration']>0][['stop_duration','expnsn_factor']].sort_values(['expnsn_factor'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd713bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgStop = pd.concat([AvgStop1, AvgStop2], axis=1)\n",
    "AvgStop_w = pd.concat([AvgStop1w, AvgStop2w], axis=1)\n",
    "AvgStop\n",
    "AvgStop_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'industry_group'\n",
    "pvt_ind_act = pd.pivot_table(df_trips, values='company_id', index=i,\n",
    "                             columns='d_act_seg_name', aggfunc='count').fillna(0) #, sort=True) For sort, default is True.\n",
    "pvt_ind_act_w = pd.pivot_table(df_trips, values='expnsn_factor', index=i,\n",
    "                             columns='d_act_seg_name', aggfunc='sum').fillna(0).round() #, sort=True) For sort, default is True.\n",
    "pvt_ind_act\n",
    "pvt_ind_act_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a103be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvt_act_plc = pd.pivot_table(df_trips, values='company_id', index='d_act_seg_name',\n",
    "                             columns='d_plc_seg_name', aggfunc='count').fillna(0) #, sort=True) For sort, default is True.\n",
    "pvt_act_plc_w = pd.pivot_table(df_trips, values='expnsn_factor', index='d_act_seg_name',\n",
    "                             columns='d_plc_seg_name', aggfunc='sum').fillna(0).round() #, sort=True) For sort, default is True.\n",
    "pvt_act_plc\n",
    "# pvt_act_plc_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a55f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = dataset + f'_StopDuration_{current_date}.xlsx'\n",
    "out_path = os.path.join(project_path, out_data_dir)\n",
    "if not os.path.exists(out_path):\n",
    "   os.makedirs(out_path)\n",
    "out_path = os.path.join(project_path, out_data_dir, out_file)\n",
    "xl_writer = pd.ExcelWriter(out_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70237495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn1 = 'StopDur_byAct&Ind'\n",
    "sn2 = 'StopDur_byAct&Ind_w'\n",
    "sn3 = 'Ind-DesAct'\n",
    "sn4 = 'Ind-DesAct_w'\n",
    "sn5 = 'DesAct-DesPlc'\n",
    "sn6 = 'DesAct-DesPlc_w'\n",
    "AvgStop.to_excel(xl_writer, sheet_name=sn1) # , index_label='index'\n",
    "AvgStop_w.to_excel(xl_writer, sheet_name=sn2) # , index_label='index'\n",
    "# pvt_ind_act.to_excel(xl_writer, sheet_name=sn3) # , index_label='index'\n",
    "# pvt_ind_act_w.to_excel(xl_writer, sheet_name=sn4) # , index_label='index'\n",
    "# pvt_act_plc.to_excel(xl_writer, sheet_name=sn5) # , index_label='index'\n",
    "# pvt_act_plc_w.to_excel(xl_writer, sheet_name=sn6) # , index_label='index'\n",
    "xl_writer.close()\n",
    "\n",
    "try:\n",
    "    df_trips.drop(columns='dur_by_weight', inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4af413",
   "metadata": {},
   "source": [
    "#### Start Creating the Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135be4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['route_id', 'industry_group', 'veh_type',\n",
    "        'trip_count', 'g_stops', 's_stops', 'm_stops', 'b_stops', 'h_stops',\n",
    "        'primary_purp', 'customer_type',\n",
    "        'start_tod', 'end_tod', 'route_dur_hr', 'cumlv_dur', 'durations_match',\n",
    "        'start_activity', 'end_activity',\n",
    "        'start_plc_seg', 'end_plc_seg',\n",
    "        'act_seg_seq', 'plc_seg_seq',\n",
    "        'headquarters', 'hq_taz',\n",
    "        'tot_distance',\n",
    "        'company_id', 'vehicle_id', 'driver_id',\n",
    "        'participation_type', 'expnsn_factor', 'estab_taz', 'trips']\n",
    "# g_stops = goods stops, s for services, m for maintenance/other, b for base, h for home. (These are super activities.)\n",
    "# Primary purpose: Goods, Services, Maintenance/Other\n",
    "# Customer type: Residential, Non-residential, Mixed\n",
    "# start_tod = starting time of day, it is the first departure; end_tod = ending time of day, it is the last arrival.\n",
    "# route_dur_hr = Duration of the route based on its start and end times.\n",
    "# cumlv_dur = Duration of the route based on trip and stay durations.\n",
    "# durations_match = True if route durations from the two methods match; else False.\n",
    "# start_activity or end_activity: Starting/ending activity at O/D\n",
    "# start_plc_seg or end_plc_seg: Starting/ending place (Base, Home, Warehouse/DC (distribution center)/Transport Node, Other)\n",
    "# headquarters: Primary point of return. Most of the time it is the base.\n",
    "# hq_taz: TAZ of the primary point of return.\n",
    "\n",
    "if dataset=='TNC': cols.insert(-1, 'company_name')\n",
    "\n",
    "df_routes = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ce0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the trip dataframe.\n",
    "for i, row in df_trips.iterrows():\n",
    "\n",
    "    # If this trip is the first trip of the route, initialize a route for it.\n",
    "    if row['trip_number'] == 1:\n",
    "        df_temp_rt = pd.DataFrame(columns=cols, index=[0])\n",
    "\n",
    "        # Generate a route ID.\n",
    "        date = str(df_trips.at[i, 'travel_date'])[:10]\n",
    "        date = date.replace('-', '')\n",
    "        date = int(date) * 10000\n",
    "        df_temp_rt['route_id'] = date + row['vehicle_id']\n",
    "\n",
    "        # Copy common attributes from the trip row to the temporary route dataframe.\n",
    "        for c in df_temp_rt.columns.to_list():\n",
    "            if c in df_trips.columns.to_list():\n",
    "                df_temp_rt[c] = row[c]\n",
    "\n",
    "        # Initialize route variables.\n",
    "        trips = []\n",
    "        activities = [row['o_act_seg']]\n",
    "        places = [row['o_plc_seg']]\n",
    "\n",
    "        # Set route variables that are known.\n",
    "        df_temp_rt['start_activity'] = row['o_act_seg']\n",
    "        df_temp_rt['start_plc_seg'] = dic_pt2[row['o_plc_seg']]\n",
    "        df_temp_rt['start_tod'] = row['o_dt']\n",
    "\n",
    "    # Initialize a trip and store the current row values into its attributes.\n",
    "    t = Trip()\n",
    "    t.index = i\n",
    "    for attr in list(vars(t).keys()):\n",
    "        if attr == 'index': continue\n",
    "        #  print(row[str(attr)])\n",
    "        #  print(getattr(t, attr))\n",
    "        setattr(t, attr, row[attr])\n",
    "\n",
    "    trips.append(t)\n",
    "    # Add trip info to the route variables.\n",
    "    activities.append(row['d_act_seg'])\n",
    "    places.append(row['d_plc_seg'])\n",
    "\n",
    "    # Finalize the route if this is the last trip of the route.\n",
    "    if row['last_trip']:\n",
    "        df_temp_rt.at[0, 'act_seg_seq'] = activities\n",
    "        df_temp_rt.at[0, 'plc_seg_seq'] = places\n",
    "        df_temp_rt.at[0, 'trips'] = trips\n",
    "\n",
    "        # Set route variables that are known.\n",
    "        df_temp_rt['end_activity'] = row['d_act_seg']\n",
    "        df_temp_rt['end_plc_seg'] = dic_pt2[row['d_plc_seg']]\n",
    "        df_temp_rt['end_tod'] = row['d_at']\n",
    "\n",
    "        # Attach the completed route to the route dataframe.\n",
    "        df_routes = pd.concat([df_routes, df_temp_rt], axis=0, ignore_index=True)\n",
    "\n",
    "#     if i == 300: break\n",
    "df_routes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_routes.iterrows():\n",
    "    # Identify number of stops by type of stop.\n",
    "    df_routes.at[i, 'trip_count'] = len(row['trips'])\n",
    "    counts = collections.Counter(row['act_seg_seq'][1:])\n",
    "    df_routes.at[i, 'g_stops'] = counts['P'] + counts['D']\n",
    "    df_routes.at[i, 's_stops'] = counts['S']\n",
    "    df_routes.at[i, 'm_stops'] = counts['M']\n",
    "    df_routes.at[i, 'b_stops'] = counts['B']\n",
    "    df_routes.at[i, 'h_stops'] = counts['H']\n",
    "\n",
    "    # Identify the primary purpose of the route.\n",
    "    if 'P' in row['act_seg_seq'][1:]:\n",
    "        purp = 'Goods'\n",
    "    elif 'D' in row['act_seg_seq'][1:]:\n",
    "        purp = 'Goods'\n",
    "    elif 'S' in row['act_seg_seq'][1:]:\n",
    "        purp = 'Service'\n",
    "    else:\n",
    "        purp = 'Maintenance/Other'\n",
    "    df_routes.at[i, 'primary_purp'] = purp\n",
    "\n",
    "    # Find total distance traveled on the route.\n",
    "    dist = 0\n",
    "    for t in row['trips']:\n",
    "        dist += t.trip_dist\n",
    "    df_routes.at[i, 'tot_distance'] = dist\n",
    "\n",
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc69ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(df_routes.at[0, 'trips']):\n",
    "    print(f'\\nTrip #{i+1}:')\n",
    "    for e in zip(list(vars(t).keys()), list(vars(t).values())):\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087af67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine customer type.\n",
    "\n",
    "# Customer Types at Stops (only applies to routes with Goods and/or Service purposes):\n",
    "# a. Residential Only (households, including multi-family buildings)\n",
    "# b. Non-residential Only (commercial, public/government)\n",
    "# c. Mixed Residential and Non-residential\n",
    "dic_cstmr_typ = {\n",
    "    'ro' : 'Residential Only',\n",
    "    'nro': 'Non-Residential Only',\n",
    "    'm'  : 'Mixed Residential and Non-residential',\n",
    "    'nc' : 'No Customer'\n",
    "}\n",
    "\n",
    "# plc_seg_code: plc_seg_name\n",
    "# 1: Residential\n",
    "# 2: Office\n",
    "# 3: Warehouse\n",
    "# 4: Other   A closer look at the trip table indicated most of these places are non-residential.\n",
    "# 5: Retail and Restaurant\n",
    "# 6: Gas\n",
    "# 7: Industrial, Agriculture, or Construction\n",
    "# 8: Truck Terminal or Parking\n",
    "\n",
    "lst_res = [1]\n",
    "lst_non_res = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Logic: if no relevant place is visited: No Customer;\n",
    "#        else if every visited place is res: Residential Only;\n",
    "#        else if there's at least one residential place that has been visited: Mixed;\n",
    "#        else: Non-Residential Only.\n",
    "\n",
    "def identify_customer(lrvp):  # lrvp is the list of relevant, visited places.\n",
    "    if not lrvp: return 'nc'  # if input list is empty, no customer has been served, or no\n",
    "                              # good has been delivered or picked up.\n",
    "    result = 'ro'\n",
    "    for plc in lrvp:\n",
    "        if plc not in lst_res:\n",
    "            result = ''\n",
    "    if result == 'ro': return result\n",
    "    result = 'm'\n",
    "    for plc in lrvp:\n",
    "        if plc not in lst_non_res:\n",
    "            return result\n",
    "    result = 'nro'\n",
    "    return result\n",
    "\n",
    "\n",
    "for i, route in df_routes.iterrows():\n",
    "    acts = route['act_seg_seq']\n",
    "    plcs = route['plc_seg_seq']\n",
    "    plcs_cleaned = []\n",
    "    for j, act in enumerate(acts):\n",
    "        if act in ['S', 'P', 'D']: plcs_cleaned.append(plcs[j])\n",
    "    cstmr = identify_customer(plcs_cleaned)\n",
    "    df_routes.at[i, 'customer_type'] = dic_cstmr_typ[cstmr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes[['act_seg_seq', 'plc_seg_seq', 'customer_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91000801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many of the routes go outside of the SANDAG TAZs.\n",
    "a = len(df_routes)\n",
    "b = len(df_routes.loc[pd.isnull(df_routes['tot_distance'])])\n",
    "print(f'{b} routes out of {a:,} routes have trips that either start or end in an external TAZ.')\n",
    "print(\"'tot_distance' field of these routes has been marked as 'NA'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e51453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what is the point of return of the route. It's either base or home.\n",
    "for i, row in df_routes.iterrows():\n",
    "    counts_act = collections.Counter(row['act_seg_seq'])\n",
    "#     counts_plc = collections.Counter(row['plc_seg_seq'])\n",
    "    if counts_act['B'] >= counts_act['H'] and counts_act['B'] > 0:\n",
    "        df_routes.at[i, 'headquarters'] = 'B'\n",
    "    elif counts_act['H'] > counts_act['B']:\n",
    "        df_routes.at[i, 'headquarters'] = 'H'\n",
    "#     elif counts_plc[3] >= counts_plc[8] and counts_plc[3] > 0:\n",
    "#         df_routes.at[i, 'headquarters'] = 'Warehouse'\n",
    "#     elif counts_plc[8] > counts_act[3]:\n",
    "#         df_routes.at[i, 'headquarters'] = 'Truck Terminal'\n",
    "    else:\n",
    "        df_routes.at[i, 'headquarters'] = 'Unknown'\n",
    "df_routes.groupby('headquarters').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes.loc[df_routes['headquarters']=='Unknown'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hq_taz(hq, lst_act, lst_trips):\n",
    "    if hq in 'BH':\n",
    "        pos = lst_act.index(hq)\n",
    "        if pos == 0:\n",
    "            return lst_trips[0].o_taz\n",
    "        else:\n",
    "            return lst_trips[pos-1].d_taz\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "df_routes['hq_taz'] = df_routes.apply(lambda x: find_hq_taz(x.headquarters, x.act_seg_seq, x.trips), axis=1)\n",
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f021be9",
   "metadata": {},
   "source": [
    "##### Now that headquarters of routes are known, go back to df_trips and update distance to headquarters for each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that gets route_id and returns hq_taz.\n",
    "dic_rid_hq_taz = dict(zip(df_routes.route_id, df_routes.hq_taz))\n",
    "\n",
    "# Specify hq_taz of trips in the trips dataframe.\n",
    "def omit_dashes(s):\n",
    "    return s.replace('-', '')\n",
    "\n",
    "df_trips['route_id'] = df_trips['travel_date'].astype(str).str[:10].apply(omit_dashes).astype('int64') * 10000\n",
    "df_trips['route_id'] += df_trips.vehicle_id\n",
    "df_trips.hq_taz = df_trips.route_id.map(dic_rid_hq_taz)\n",
    "\n",
    "df_trips[['travel_date', 'vehicle_id', 'route_id', 'hq_taz']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8898a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add route primary purposes to trips\n",
    "dic_rte_purp = dict(zip(df_routes.route_id, df_routes.primary_purp))\n",
    "df_trips['route_purpose'] = df_trips.route_id.map(dic_rte_purp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da799a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add route primary customer type to trips\n",
    "dic_rte_cust = dict(zip(df_routes.route_id, df_routes.customer_type))\n",
    "df_trips['route_customers'] = df_trips.route_id.map(dic_rte_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update distance to headquarters.\n",
    "df_trips['orgn_to_hq_dist'] = df_trips.apply(lambda x: find_dist(x['o_taz'], x['hq_taz'],\n",
    "                                                           dic_v[x['veh_type']],\n",
    "                                                           dic_tod[x['tod']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc41666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b413b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_trips.groupby(['route_purpose']).size().to_string(), \"\\n\")\n",
    "print(df_trips.groupby(['route_customers']).size().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fba51a",
   "metadata": {},
   "source": [
    "####  Run a Few Checks for Quality Control of the Route Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the route start to end duration in minutes.\n",
    "df_routes['route_dur_min'] = ((df_routes['end_tod'] - df_routes['start_tod'])\n",
    "                             .dt.total_seconds()/60).round()\n",
    "\n",
    "# Find the route duration in minutes based on its trips and stays durations.\n",
    "for i, route in df_routes.iterrows():\n",
    "    duration = 0\n",
    "    trips = route['trips']\n",
    "    for j, t in enumerate(trips):\n",
    "        if j != len(trips)-1:\n",
    "            duration += (t.travel_time + t.stop_duration)\n",
    "        else:\n",
    "            duration += t.travel_time\n",
    "    duration = round(duration)\n",
    "    df_routes.at[i, 'cumlv_dur'] = duration\n",
    "\n",
    "    # Specify if route durations from the two methods are off by more than tolerance minutes.\n",
    "    tol = 15\n",
    "    if abs(duration - route['route_dur_min']) >= tol:\n",
    "        df_routes.at[i, 'durations_match'] = False\n",
    "    else:\n",
    "        df_routes.at[i, 'durations_match'] = True\n",
    "\n",
    "misses = len(df_routes.loc[df_routes['durations_match'] == False])\n",
    "matches = len(df_routes.loc[df_routes['durations_match'] != False])\n",
    "print(f\"Number of routes with duration matches = {matches}; misses = {misses} (tolerance = {tol} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes.loc[df_routes['durations_match'] == False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb73813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if the route has extended beyond the first day. This does not require that the duration of a route is more\n",
    "# than 24 hours, rather if the first departure is in one calendar day while the last arrival is in another day, the\n",
    "# route will be flagged as Multi-Day.\n",
    "try:\n",
    "    df_routes.insert(13, 'multiday_route', False)\n",
    "except:\n",
    "    pass\n",
    "df_routes['multiday_route'] = df_routes['start_tod'].dt.date < df_routes['end_tod'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes.loc[df_routes['multiday_route']==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that indicates if the route has any warning.\n",
    "df_routes['has_warning'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f10d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify routes that vehicle goes from Base to Base, or it goes from Home to Home.\n",
    "for i, route in df_routes.iterrows():\n",
    "    flag = False\n",
    "    acts = route['act_seg_seq']\n",
    "    act = acts[0]\n",
    "    for next_act in acts[1:]:\n",
    "        if (act=='H' and next_act=='H') or (act=='B' and next_act=='B'):\n",
    "            flag = True\n",
    "            break\n",
    "        act = next_act\n",
    "    df_routes.at[i, 'warn_BB_or_HH'] = flag*1\n",
    "df_routes['warn_BB_or_HH'] = df_routes['warn_BB_or_HH'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ad01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add warning columns.\n",
    "\n",
    "# Activity at the route origin is to deliver/pickup goods:\n",
    "df_routes['warn_o_act_G'] = df_routes['act_seg_seq'].apply(lambda l: 1 if l[0]=='G' else 0)\n",
    "\n",
    "# Activity at the route origin is to provide services:\n",
    "df_routes['warn_o_act_S'] = df_routes['act_seg_seq'].apply(lambda l: 1 if l[0]=='S' else 0)\n",
    "\n",
    "# Activity at the route destination is either to provide service or deliver/pickup goods:\n",
    "df_routes['warn_d_act_SG'] = df_routes['act_seg_seq'].apply(lambda l: 1 if l[-1] in ['G', 'S'] else 0)\n",
    "\n",
    "# Route trips extend beyond 12AM of the first day:\n",
    "df_routes['warn_next_day'] = df_routes['multiday_route'].astype(int)\n",
    "\n",
    "# Route duration time calculated from start and end of the tour doesn't match with the one that is calculated by\n",
    "# summing trip travel times and stop durations:\n",
    "df_routes['warn_duration'] = 1 - df_routes['durations_match'].astype(int)\n",
    "\n",
    "# Neither Base nor Home appears in the route activities, including the route origin:\n",
    "df_routes['warn_no_BH_stops'] = df_routes['act_seg_seq'].apply(lambda l: 0 if ('B' in l or 'H' in l) else 1)\n",
    "\n",
    "# Neither Goods nor Services appears in the route activities, including the route origin:\n",
    "df_routes['warn_no_GS_stops'] = df_routes['act_seg_seq'].apply(lambda l: 0 if ('G' in l or 'S' in l) else 1)\n",
    "\n",
    "# None of Goods, Services, or Maintenance appears in the route activities, including the route origin:\n",
    "df_routes['warn_no_GSM_stops'] = df_routes['act_seg_seq'].apply(lambda l: 0 if ('G' in l or 'S' in l or 'M' in l) else 1)\n",
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the has_warning column.\n",
    "warn_cols = [c for c in df_routes.columns if 'warn' in c]\n",
    "df_routes['has_warning'] = df_routes[warn_cols].max(axis=1)\n",
    "# df_routes.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b85d2",
   "metadata": {},
   "source": [
    "#### Explore the Route Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0936edd",
   "metadata": {},
   "source": [
    "##### OPTIONAL: Create a cumulative distribution function for route durations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d843fab7",
   "metadata": {},
   "source": [
    "durs = pd.Series(np.ceil(df_routes['route_dur_min']*4)/4)\n",
    "print(durs.count())\n",
    "durs.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cbf0763",
   "metadata": {},
   "source": [
    "counts = collections.Counter(durs)\n",
    "# print(counts, '\\n')\n",
    "# print(dict(counts))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ac328ce",
   "metadata": {},
   "source": [
    "df_durations = pd.DataFrame({'hr_bin': dict(counts).keys(), 'counts': dict(counts).values()})\n",
    "df_durations.sort_values('hr_bin', inplace=True)\n",
    "df_durations.reset_index(drop=True, inplace=True)\n",
    "df_durations.head(8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8036c534",
   "metadata": {},
   "source": [
    "s = df_durations['counts'].sum()\n",
    "df_durations['perc'] = (100 * df_durations['counts'] / s)\n",
    "df_durations['cum_perc'] = df_durations['perc'].cumsum()\n",
    "df_durations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2c931d4",
   "metadata": {},
   "source": [
    "plt.plot(df_durations['hr_bin'], df_durations['cum_perc'])\n",
    "plt.xlabel('Route Duration (hours)')\n",
    "plt.ylabel('Cumulative Percentage')\n",
    "plt.title('CDF of Route Duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42cb4e",
   "metadata": {},
   "source": [
    "##### OPTIONAL: Create a graph for starting/ending activity combinations.\n",
    "Note: Starting activity is the activity at the origin of the first trip, not at its destination."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b61261a",
   "metadata": {},
   "source": [
    "strt_end = pd.Series(df_routes['start_activity'] + df_routes['end_activity'])\n",
    "strt_end.head(4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a7326ed",
   "metadata": {},
   "source": [
    "counts = collections.Counter(strt_end)\n",
    "df_se = pd.DataFrame({'strt_end': dict(counts).keys(), 'counts': dict(counts).values()})\n",
    "df_se.sort_values('counts', ascending=False, inplace=True)\n",
    "df_se.reset_index(drop=True, inplace=True)\n",
    "df_se.head(8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e2a859e",
   "metadata": {},
   "source": [
    "s = df_se['counts'].sum()\n",
    "df_se['perc'] = (100 * df_se['counts'] / s).round(1)\n",
    "df_se.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a91c764e",
   "metadata": {},
   "source": [
    "ax = df_se.plot.bar(x='strt_end', y='perc', width=0.8)\n",
    "plt.xlabel('OD Activity Combination')\n",
    "plt.ylabel('Percentage of Routes')\n",
    "plt.title('Activity Combinations at origin and destination of the Routes')\n",
    "\n",
    "# Add value annotations on top of each bar.\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "# plt.figure(figsize=(200, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8addd065",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_se.sort_values('counts', ascending=True, inplace=True)\n",
    "highlight_color = '#1b9e77' # Green\n",
    "non_highlight_color = '#768493' # Gray\n",
    "# non_highlight_color = '#d95f02' # Orange\n",
    "\n",
    "df_se['color'] = df_se['perc'].apply(lambda x: highlight_color if x > 4 else non_highlight_color)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "bars = plt.barh(df_se['strt_end'], df_se['perc'],\n",
    "                height=0.7, color=df_se['color']) # height indicates thickness of bars.\n",
    "\n",
    "# Remove the top, bottom and right edges of the chart.\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "\n",
    "# Remove the numbers on the axis.\n",
    "ax.xaxis.set_visible(False)\n",
    "\n",
    "# Add data labels.\n",
    "# ax.bar_label(bars)\n",
    "\n",
    "# Control the label format of matplotlib’s bar_label function.\n",
    "ax.bar_label(bars, color='black', fontsize=11, label_type='edge',\n",
    "             fmt='{:4.1f}%', padding=+4) # padding=-45, fmt='%.1f%%', fontweight='bold'\n",
    "\n",
    "# Add the title.\n",
    "ax.set_title('Activity Combinations at origin and destination of the Routes', fontsize=12,\n",
    "              fontweight='bold', pad=10)\n",
    "\n",
    "\n",
    "# # Add a text annotation to show the basis of highlighting.\n",
    "# ax.yaxis.set_tick_params(labelsize=14)\n",
    "# ax.axvline(x=4, zorder=0, color='grey', ls='--', lw=1.5)\n",
    "# ax.text(x=4, y=1, s='4 percent threshold', ha='left',\n",
    "#         fontsize=11, bbox=dict(facecolor='white', edgecolor='grey', ls='--'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b0815",
   "metadata": {},
   "source": [
    "#### Report Out the Trip Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(df_trips):,} trips in the dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange to have time-related fields next to each other.\n",
    "cols = ['industry_group', 'trip_number', 'last_trip',\n",
    "        'travel_date', 'o_dt', 'tod', 'd_at', 'nt_dt', 'travel_time', 'stop_duration',\n",
    "        'o_act_seg', 'o_act_seg_name', 'o_plc_seg', 'o_plc_seg_name',\n",
    "        'd_act_seg', 'd_act_seg_name', 'd_plc_seg', 'd_plc_seg_name',\n",
    "        'o_act', 'o_place_type', 'o_taz',\n",
    "        'd_act', 'd_place_type', 'd_taz',\n",
    "        'trip_dist',\n",
    "        'veh_type',\n",
    "        'route_id', 'company_id', 'estab_taz', 'vehicle_id', 'driver_id',\n",
    "        'expnsn_factor', 'participation_type', 'route_purpose', 'route_customers']\n",
    "\n",
    "if dataset=='TNC':\n",
    "    cols.insert(-5, 'company_name')\n",
    "\n",
    "df_trips = df_trips[cols]\n",
    "df_trips.head(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_trips.drop(columns=['headquarters', 'hq_taz'], inplace=True)\n",
    "df_trips.rename(columns={'orgn_to_hq_dist': 'dist_to_base_mile'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = dataset + f'_Trips_{current_date}.xlsx'\n",
    "out_path = os.path.join(project_path, out_data_dir, out_file)\n",
    "df_trips.to_excel(out_path, sheet_name='Trip Dataframe', index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Out the Route Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['multiday_route', 'cumlv_dur', 'durations_match', 'trips', 'headquarters', 'hq_taz']\n",
    "df_routes.drop(columns=drop_cols, inplace=True)\n",
    "df_routes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = dataset + f'_Routes_{current_date}.xlsx'\n",
    "out_path = os.path.join(project_path, out_data_dir, out_file)\n",
    "df_routes.to_excel(out_path, sheet_name='Routes', index=False) #  index_label='index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
